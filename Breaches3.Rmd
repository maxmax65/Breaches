

---
title: "Breaches Analysis"
output: 
  html_document:
    keep_md: true
---

```{r "Housekeeping", echo=FALSE}
        ##get ggplot package
        require(ggplot2)
        require(xtable)
```

###Introduction

This is an analysis of information security breach data from the website [Information is Beautiful](http://www.informationisbeautiful.net/). They have made a really elegant bubble chart showing a very intuitive time evolution of data breaches which can be encoded for a number of factors. 

Thankfully they have also made their raw data compilation [available here](https://docs.google.com/spreadsheet/ccc?key=0AmenB57kGPGKdHh6eGpTR2lPQl9NZmo3RlVzQ1N2Ymc&single=true&gid=2&range=A1%3AW400#gid=2) so I've used it for this analysis to expand on some specific questions I wanted to address.  


The specific question I want to address here (quantitatively) is _"How much have the severity and number of hacking-induced data breaches changed over time and is it different than the general trend of breaches."_  

The conclusion is that while Hacks are increasing faster than the overall rate of breaches, the sensitivity of the data lost to hacks is not increasing as fast as that of breaches overall.

###Cleaned Data  

The data behind the Information is Beautiful chart are made available as a google doc. I've been unable (so far) to write a successful R program to download the data from the website directly, so in the interest of time I just downloaded their raw data as a .csv.  

I made the download copy of the data I use below available on the Github repository linked to this analysis. 

```{r "get_data", echo=FALSE}
#


## tried to automate a download of the google docs spreadsheet from this link
##download.file("http://docs.google.com/spreadsheet/ccc?key=0AmenB57kGPGKdHh6eGpTR2lPQl9NZmo3RlVzQ1N2Ymc&single=true&gid=2&range=A1%3AW400#gid=2", "test.csv")
##but was unable to get R to get it. 
##That would be a good addition to the program. 

## Get the data

        Breaches <- read.csv("Balloon Race- Data Breaches - Public Data - Visualisation Data.csv", header=TRUE)

```

The data was generally in pretty good shape, but I did need to take some steps to clean it up for analysis in R.

Steps to clean the data include:
* Retaining only spreadsheet columns 1-10 to include only data I need.  
* Getting rid of a second descriptive row (which is not data).  
* converting column names to lower case and deleting spaces
* cleaning some formatting of numbers to remove commas etc.   
* Implementing some assumptions about the coding of severities. The legend runs between 1 (least severe) and 5 (most severe). Some of the data are outside this range (e.g. 20 and 50000). Based on my reading of details, I have assumed these are incorrect and have converted them to single digits.  
* Turned the year into actual numerical representation of a calendar year.  

You can see the details in the code.  

```{r, "data_cleaning", results="asis", echo=FALSE}

## DATA CLEANING


        ## delete trailing columns
        Breaches<-Breaches[,1:10]
  
        ##get rid of second desrciptive row
        testt<-as.vector(Breaches[1,] )

                ##function applying as.character to a list   
                tc<-function(x) as.character(x)

        #manage column names
                testt<-lapply(testt, tc)
                ##apply colnames
                testt<-tolower(testt)
                testt<-gsub(" ", "", testt)

                colnames(Breaches)<-testt
        
        ##delete junk rows
        Breaches<-Breaches[c(-1, -2),]

        ##get rid of commas in some numerical recorgs
        tempo <- sub("[,]", "", as.character(Breaches$noofrecordsstolen))
        #Breaches$NO.OF.RECORDS.STOLEN <- sub("[,]", "", as.character(Breaches$NO.OF.RECORDS.STOLEN))

        #Make the year numeric
        Breaches$year<-as.integer(as.numeric(as.character(Breaches$year))+2004)      

        ##Make Breaches numeric
        Breaches$noofrecordsstolen<-as.integer(as.character(Breaches$noofrecordsstolen))



##clean up sensitivity
## Note that this is a little subjective. The legend ranges from 1 to 5, but the data goes to 5000 and sometimes contains multiple impacts
## I have just taken the first character of each line. This will tend to underestimate impact but is reproducible from a first pass effort.


##Turn sensitivity into character
Breaches$datasensitivity<-as.character(Breaches$datasensitivity)
##create function to select first character
first.char <- function(x){
        substring(x, 1, 1)
}
##apply the function
Breaches$datasensitivity <- lapply(Breaches$datasensitivity, first.char)
##turn back into numeric
Breaches$datasensitivity<-as.numeric(Breaches$datasensitivity)


```

Here is a random subset of the cleaned data. 
You'll note that one row has blank data for number of records stolen. I'm not sure why that is. I plan to look into it later but don't have time right now. Judging from a back-of-the-envelope calculation I don't think it biases the conclusions of this analysis.  


```{r "printtable", results="asis", echo=FALSE}

        ##Print a few random lines and select columns of data
        
                ##can set random seed and create a consistent list of elements if desired
                set.seed(11235)
                ##create elements list
                relem <- sample(1:dim(Breaches)[1],8,replace=FALSE)
                relem <- c(1:2, relem)
                ##create the table to print
                example<-xtable(Breaches[sort(relem),c(1,4,7,9,10)])
                ##print as html
                print(example, type="html", size="small")  



```

###Aggregate Records Lost  

The trend of total records lost from all data breaches shows a general upward trend. The plot below is a full aggregate of all the data. There is substantial year on year variation, dominantly from the fact that large data breaches are (thankfully) still rare from the standpoint that the 1/sqrt(N) is reasonably large. 

```{r "AggRecordsLost", echo=FALSE}


AggBreaches <- aggregate(Breaches$noofrecordsstolen, list(Year = Breaches$year), sum, na.rm=TRUE)

colnames(AggBreaches) <-c("Year", "Records_Stolen")

par(mar=c(6,5,5,3))

p1<-ggplot(AggBreaches, aes(x=Year, y=Records_Stolen)) +
        geom_point(shape=2, col=2) +
        geom_smooth(method=lm)+
        ggtitle("Total Records Lost from All Breaches") +
        theme(text = element_text(size=14))


print(p1)

```


###Records Stolen by Hacking  

An interesting segmentation is to look at the records stolen by hacking (as opposed to, say, physical theft or an insider). Hacking is usually associated with malicious outside attack of a vulnerability. The industry is taking increased steps to close vulnerabilities, but the investment in hacks, especially by nation-states and organized crime, is also rising. 


```{r "Hacks", echo=FALSE}

#Create Hacks File
Hacks<-Breaches[grep("hacked", Breaches$methodofleak),]
##aggregate hacks data as a sum
AggHacks <- aggregate(Hacks$noofrecordsstolen, list(Year = Hacks$year), sum, na.rm=TRUE)

colnames(AggHacks) <-c("Year", "Records_Stolen")

p2<-ggplot(AggHacks, aes(x=Year, y=Records_Stolen)) +
        geom_point(shape=3, col=1) +
        geom_smooth(method=lm)+
        ggtitle("Total Records Lost from Hacking Breaches") +
        theme(text = element_text(size=14))

print(p2)

```

```{r "slopes", echo=FALSE}

Breachlm <- lm(Records_Stolen~Year, data = AggBreaches)
Hacklm <- lm(Records_Stolen~Year, data = AggHacks)

#summary(BreachSenslm)

BreachPerYear<-as.integer(Breachlm$coefficient[2]/1000)
HackPerYear<-as.integer(Hacklm$coefficient[2]/1000)

HBratio = signif(100.*(1.-BreachPerYear/HackPerYear),2)


```


As can be seen from the data, breaches from hacks vary significantly from year to year, but also have resulted in a generally upward trend of number of records being lost. The fitted trend line apparently rises faster than the trend line for the overall breaches. Indeed the rate of increase inrecords lost  `r HackPerYear` thousand/year is about `r HBratio` % higher than `r BreachPerYear` thousand/year records lost from all breaches.  

###Average severity of Records Stolen by Hacking versus the overall trend  


```{r "Multiplot", echo=FALSE}
## I lifted this from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
        require(grid)
        
        # Make a list from the ... arguments and plotlist
        plots <- c(list(...), plotlist)
        
        numPlots = length(plots)
        
        # If layout is NULL, then use 'cols' to determine layout
        if (is.null(layout)) {
                # Make the panel
                # ncol: Number of columns of plots
                # nrow: Number of rows needed, calculated from # of cols
                layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                                 ncol = cols, nrow = ceiling(numPlots/cols))
        }
        
        if (numPlots==1) {
                print(plots[[1]])
                
        } else {
                # Set up the page
                grid.newpage()
                pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
                
                # Make each plot, in the correct location
                for (i in 1:numPlots) {
                        # Get the i,j matrix positions of the regions that contain this subplot
                        matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
                        
                        print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                                        layout.pos.col = matchidx$col))
                }
        }
}
```

The record loss from hacks is increasing, as shown above, faster than the total of data breaches, but is the severity of the attacks also increasing?  

The data is categorized by severity on a scale of 1-5.   
1. Just email address/Online information   
2. SSN/Personal details   
3. Credit card information   
4. Email password/Health records   
5. Full bank account details  

The scale is of course arbitrary, but it is sensible and provides a systematic framework for measuring qualitative change. 

The analysis to compute the mean severity of attacks is straigtforward. The results, shown below, show that in fact while the severity of hacks are increasing, they are not increasing as fast as the severity of all attacks. 

```{r "Severity", echo=FALSE}

#Create Hacks File
Hacks<-Breaches[grep("hacked", Breaches$methodofleak),]
##aggregate hacks data as a mean
AggHackSens <- aggregate(Hacks$datasensitivity, list(Year = Hacks$year), mean, na.rm=TRUE)

colnames(AggHackSens) <-c("Year", "Data_Sensitivity")

p1<-ggplot(AggHackSens, aes(x=Year, y=Data_Sensitivity)) +
        geom_point(shape=3, col=1) +
        geom_smooth(method=lm)+
        ggtitle("Mean Sensitivity from Hacks") +
        theme(text = element_text(size=14))

```

```{r, "AggBreachSens", echo=FALSE}


##aggregate Breaches data as a mean
AggBreachSens <- aggregate(Breaches$datasensitivity, list(Year = Breaches$year), mean, na.rm=TRUE)

colnames(AggBreachSens) <-c("Year", "Data_Sensitivity")

p2<-ggplot(AggBreachSens, aes(x=Year, y=Data_Sensitivity)) +
        geom_point(shape=2, col=2) +
        geom_smooth(method=lm)+
        ggtitle("Mean Sensitivity from All Breaches") +
        theme(text = element_text(size=14))


multiplot(p1, p2, cols=2)

BreachSenslm <- lm(Data_Sensitivity~Year, data = AggBreachSens)
HackSenslm <- lm(Data_Sensitivity~Year, data = AggHackSens)


```

```{r "Slope_Stats", echo=FALSE}

#summary(BreachSenslm)

BreachSensPerYear<-BreachSenslm$coefficient[2]/1000.
HackSensPerYear<-HackSenslm$coefficient[2]/1000.

HBSenRatio <- signif(1.*BreachSensPerYear/HackSensPerYear,2)

```


So in response to the orginial question:  

_How much have the severity and number of hacking-induced data breaches changed over time and is it different than the general trend of breaches._ 

While number of hacking-induced data record losses has increased about `r HBratio` % per year faster than the number of records lost from breaches overall, we find the surprising result that the sensitivity of records lost from breaches overall is increasing `r HBSenRatio` times faster than the sensitivity of Hacks. 

This means that while the number of hacked records is growing, the biggest problem from a data sensitivty standpoint is actually from other causes. 

The _number_ of records lost to hacks are increasing faster than the overall rate of record losses from data breaches, the _sensitivity_ of the data stolen by hackers is atually increasing more slowly than that from breaches overall.   

A couple of hypothesis:  
1. Hacking attacks, such as the large breach at Target or Home Depot, focused on stealing credit card numbers, presumably for financial gain. These activities, while getting larger, will be concentrated around sensitivity level 3. They are not after medical records in general.   
2. HIPAA now requires disclosure of breaches of medical records in some cases. Medical records on the scale used here, have higher sensitivity. So there may be a systematic bias in which data are reported that is affecting the overall trend.   

... which suggest questions for the next round a programming.
